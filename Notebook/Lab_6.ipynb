{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LAB 06 - Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luca Catalano, Daniele Rege Cambrin, Eleonora Poeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The purpose of creating this material is to enhance the knowledge of students who are interested in learning how to solve problems presented in laboratory classes using Python. This decision stems from the observation that some students have opted to utilize Python for tackling exam projects in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve these exercises using Python, you need to install Python (version 3.9.6 or later) and some libraries using pip or conda.\n",
    "\n",
    "Here's a list of the libraries needed for this case:\n",
    "\n",
    "- `os`: Provides operating system dependent functionality, commonly used for file operations such as reading and writing files, interacting with the filesystem, etc.\n",
    "- `pandas`: A data manipulation and analysis library that offers data structures and functions to efficiently work with structured data.\n",
    "- `numpy`: A numerical computing library that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "- `matplotlib.pyplot`: A plotting library for creating visualizations like charts, graphs, histograms, etc.\n",
    "- `sklearn`: Machine learning algorithms and tools.\n",
    "- `xlrd`: A Python library used for reading data and formatting information from Excel files (.xls and .xlsx formats). It provides functionality to extract data from Excel worksheets, including cells, rows, columns, and formatting details.\n",
    "\n",
    "You can download Python from [here](https://www.python.org/downloads/) and follow the installation instructions for your operating system.\n",
    "\n",
    "For installing libraries using [pip](https://pip.pypa.io/en/stable/) or [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html), you can use the following commands:\n",
    "\n",
    "- Using pip:\n",
    "  ```\n",
    "  pip install pandas numpy matplotlib scikit-learn xlrd\n",
    "  ```\n",
    "\n",
    "- Using conda:\n",
    "  ```\n",
    "  conda install pandas numpy matplotlib scikit-learn xlrd\n",
    "  ```\n",
    "\n",
    "Make sure to run these commands in your terminal or command prompt after installing Python. You can also execute them in a cell of a Jupyter Notebook file (`.ipynb`) by starting the command with '!'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel \"user.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the label column in the dataset data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the 'Response' column to 'Label' [use dataset.rename(columns={'actual_col_name': 'new_col_name'})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column Response to Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datsaset to check if the column has been renamed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Separate the dataset into features, referred to as X, and labels, referred to as y. Afterwards, utilize Label Encoder to encode the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[You can achieve this by selecting columns using the [] operator on the dataframe, then initializing the Label Encoder and applying its fit_transform method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "# Features\n",
    "\n",
    "# Target variable\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "\n",
    "# Apply label encoding to each column, except for the age column\n",
    "\n",
    "# Transform Negative into 0value and Positive into 1 value (use label encoder with .fit_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the random forest classifier model.\n",
    "\n",
    "To start, split the dataset `users.xlsx` into two parts: training and testing. This allows for training the model on the training portion and evaluating its performance using the test portion.\n",
    "\n",
    "Please note that the test portion is not a real-case test dataset but rather an archetype for evaluating the model with a small dataset that contains the correct labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these parameters:\n",
    "\n",
    "- Max Depth: 100\n",
    "- Number of trees: 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[use train_test_split() to split the dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use RandomForestClassifier() and its .fit and .predict function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "# Predict the response for test dataset\n",
    "\n",
    "\n",
    "# Evaluate the model: Accuracy, Precision, Recall\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of Random Forest Classifier model using Cross Validation\n",
    "\n",
    "Cross-validation is a technique used to assess the performance and generalization ability of machine learning models, particularly in the context of classification tasks. It involves partitioning the dataset into multiple subsets, known as folds.\n",
    "\n",
    "1. **Partitioning the Dataset**: The dataset is divided into k equal-sized folds.\n",
    "\n",
    "2. **Training and Testing**: The model is trained k times, each time using k-1 folds for training and the remaining fold for testing.\n",
    "\n",
    "3. **Evaluation**: The performance of the model is evaluated on each fold, and the results are averaged to obtain a robust estimate of the model's performance.\n",
    "\n",
    "4. **Advantages**: Cross-validation provides a more reliable estimate of the model's performance compared to a single train-test split. It helps to detect overfitting and assesses the model's ability to generalize to unseen data.\n",
    "\n",
    "[Use `cross_val_score` and `cross_val_predict` to perform cross-validation easily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree classifier\n",
    "\n",
    "\n",
    "# Perform cross-validation predictions\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "\n",
    "\n",
    "# Evaluate accuracy\n",
    "\n",
    "\n",
    "# Print accuracy\n",
    "\n",
    "\n",
    "# Print confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implement Grid Search\n",
    "\n",
    "Grid Search is a technique used to find the optimal hyperparameters for a machine learning model. It works by searching through a predefined set of hyperparameters and evaluating the model's performance for each combination using cross-validation.\n",
    "\n",
    "\n",
    "Specifically, you need to:\n",
    "\n",
    "1. Define a grid of hyperparameters to search through.\n",
    "2. Use Grid Search to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search. It takes more or less 30 seconds to run\n",
    "# Define the parameter grid\n",
    "param_grid =    {\n",
    "                    \"n_estimators\": [100, 250, 500],\n",
    "                    \"max_depth\": [None, 10, 20, 30],\n",
    "                }\n",
    "# Perform grid search\n",
    "\n",
    "# Initialize the grid search [use .fit() method]\n",
    "\n",
    "# Print the best parameters and the best score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC as SupportVectorMachineClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel \"user.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the label column in the dataset data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the 'Response' column to 'Label' [use dataset.rename(columns={'actual_col_name': 'new_col_name'})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column Response to Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datsaset to check if the column has been renamed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Separate the dataset into features, referred to as X, and labels, referred to as y. Afterwards, utilize Label Encoder to encode the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[You can achieve this by selecting columns using the [] operator on the dataframe, then initializing the Label Encoder and applying its fit_transform method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "# Features\n",
    "\n",
    "# Target variable\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "\n",
    "# Apply label encoding to each column, except for the age column\n",
    "\n",
    "\n",
    "# Transform Negative into 0value and Positive into 1 value (use label encoder with .fit_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Support Vector Machine classifier model.\n",
    "\n",
    "Use the same split of the dataset `users.xlsx` into two parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these parameters:\n",
    "\n",
    "- C: 100\n",
    "- gamma: 0.1\n",
    "- kernel='rbf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use SVM() and its .fit and .predict function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training set and test set\n",
    "\n",
    "\n",
    "# Create a SVM Classifier\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "# Predict the response for test dataset\n",
    "\n",
    "\n",
    "# Evaluate the model: Accuracy, Precision, Recall\n",
    "\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of SVM Classifier model using Cross Validation\n",
    "\n",
    "Cross-validation is a technique used to assess the performance and generalization ability of machine learning models, particularly in the context of classification tasks. It involves partitioning the dataset into multiple subsets, known as folds.\n",
    "\n",
    "1. **Partitioning the Dataset**: The dataset is divided into k equal-sized folds.\n",
    "\n",
    "2. **Training and Testing**: The model is trained k times, each time using k-1 folds for training and the remaining fold for testing.\n",
    "\n",
    "3. **Evaluation**: The performance of the model is evaluated on each fold, and the results are averaged to obtain a robust estimate of the model's performance.\n",
    "\n",
    "4. **Advantages**: Cross-validation provides a more reliable estimate of the model's performance compared to a single train-test split. It helps to detect overfitting and assesses the model's ability to generalize to unseen data.\n",
    "\n",
    "[Use `cross_val_score` and `cross_val_predict` to perform cross-validation easily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decision tree classifier\n",
    "\n",
    "\n",
    "# Perform cross-validation predictions\n",
    "\n",
    "\n",
    "# Calculate confusion matrix\n",
    "\n",
    "\n",
    "# Evaluate accuracy\n",
    "\n",
    "\n",
    "# Print accuracy\n",
    "\n",
    "\n",
    "# Print confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implement Grid Search\n",
    "\n",
    "Grid Search is a technique used to find the optimal hyperparameters for a machine learning model. It works by searching through a predefined set of hyperparameters and evaluating the model's performance for each combination using cross-validation.\n",
    "\n",
    "\n",
    "Specifically, you need to:\n",
    "\n",
    "1. Define a grid of hyperparameters to search through.\n",
    "2. Use Grid Search to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search. It takes more or less 30 seconds to run\n",
    "# Define the parameter grid\n",
    "param_grid =    {\n",
    "                    \"C\": [1, 2, 5, 10],\n",
    "                    \"gamma\": [2, 1, 0.1, 0.01],\n",
    "                    \"kernel\": ['rbf', 'linear']\n",
    "                }\n",
    "# Perform grid search\n",
    "\n",
    "# Initialize the grid search\n",
    "\n",
    "\n",
    "# Print the best parameters and the best score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel \"user.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define the label column in the dataset data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the 'Response' column to 'Label' [use dataset.rename(columns={'actual_col_name': 'new_col_name'})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column Response to Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print datsaset to check if the column has been renamed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Separate the dataset into features, referred to as X, and labels, referred to as y. Afterwards, utilize Label Encoder to encode the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[You can achieve this by selecting columns using the [] operator on the dataframe, then initializing the Label Encoder and applying its fit_transform method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "# Features\n",
    "\n",
    "# Target variable\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "\n",
    "# Apply label encoding to each column, except for the age column\n",
    "\n",
    "\n",
    "# Transform Negative into 0value and Positive into 1 value (use label encoder with .fit_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the MLP classifier model.\n",
    "\n",
    "Use the same split of the dataset `users.xlsx` into two parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network (ANN) that consists of multiple layers of nodes, or neurons, arranged in a feedforward manner. MLPs are widely used for various machine learning tasks, including classification and regression.\n",
    "\n",
    "### Structure of an MLP:\n",
    "\n",
    "1. **Input Layer**: The first layer of the MLP, which receives input features from the dataset.\n",
    "\n",
    "2. **Hidden Layers**: Intermediate layers between the input and output layers. Each hidden layer consists of multiple neurons, and the number of hidden layers and neurons per layer can vary depending on the complexity of the task.\n",
    "\n",
    "3. **Output Layer**: The final layer of the MLP, which produces the network's output. The number of neurons in the output layer depends on the number of classes in the classification task or the number of output values in the regression task.\n",
    "\n",
    "### Activation Function:\n",
    "\n",
    "Each neuron in the MLP applies an activation function to its input to introduce non-linearity into the model and enable the network to learn complex patterns. Common activation functions include:\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**\n",
    "- **Sigmoid**\n",
    "- **Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "### Training an MLP:\n",
    "\n",
    "MLPs are trained using an optimization algorithm such as gradient descent to minimize a loss function, which measures the difference between the predicted output and the true labels in the training data. Common loss functions include cross-entropy loss for classification tasks and mean squared error for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set these parameters:\n",
    "\n",
    "- max_iter = 500 \n",
    "- solver='sgd' \n",
    "- learning_rate_init=0.001\n",
    "- hidden_layer_sizes=(512, 256, 128)\n",
    "- random_state=42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Use MLPClassifier() and its .fit and .predict function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "\n",
    "# Create a MLP Classifier\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "# Predict the response for test dataset\n",
    "\n",
    "# Evaluate the model: Accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
