{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LAB 04 - Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luca Catalano, Daniele Rege Cambrin and Eleonora Poeta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The purpose of creating this material is to enhance the knowledge of students who are interested in learning how to solve problems presented in laboratory classes using Python. This decision stems from the observation that some students have opted to utilize Python for tackling exam projects in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve these exercises using Python, you need to install Python (version 3.9.6 or later) and some libraries using pip or conda.\n",
    "\n",
    "Here's a list of the libraries needed for this case:\n",
    "\n",
    "- `os`: Provides operating system dependent functionality, commonly used for file operations such as reading and writing files, interacting with the filesystem, etc.\n",
    "- `pandas`: A data manipulation and analysis library that offers data structures and functions to efficiently work with structured data.\n",
    "- `numpy`: A numerical computing library that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "- `matplotlib.pyplot`: A plotting library for creating visualizations like charts, graphs, histograms, etc.\n",
    "- `sklearn`: Machine learning algorithms and tools.\n",
    "- `sklearn_extra`: Additional machine learning algorithms and extensions.\n",
    "- `nltk`: The Natural Language Toolkit, a library for natural language processing tasks such as tokenization, stemming, part-of-speech tagging, and more.\n",
    "- `xlrd`: A Python library used for reading data and formatting information from Excel files (.xls and .xlsx formats). It provides functionality to extract data from Excel worksheets, including cells, rows, columns, and formatting details.\n",
    "\n",
    "You can download Python from [here](https://www.python.org/downloads/) and follow the installation instructions for your operating system.\n",
    "\n",
    "For installing libraries using [pip](https://pip.pypa.io/en/stable/) or [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html), you can use the following commands:\n",
    "\n",
    "- Using pip:\n",
    "  ```\n",
    "  pip install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "- Using conda:\n",
    "  ```\n",
    "  conda install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "Make sure to run these commands in your terminal or command prompt after installing Python. You can also execute them in a cell of a Jupyter Notebook file (`.ipynb`) by starting the command with '!'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file excel named \"UserSmall.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle Missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find if there are missing values in our dataset. \n",
    "\n",
    "Usually in a real dataset the missing values are stored with a nan value. In this case we have ? as missing values representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all we can replace each '?' symbol in a nan value [using .replace()]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values for each column [using .isnumm() and .sum() functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in class there are different methodologies for filling the nan values. Here we will use the average for the numerical data and the most frequent string for non-numerical columns [use .fillna() and mean() function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with the average value for numerical columns\n",
    "    # Get the average value for the column and replace NaN values with it\n",
    "\n",
    "# Replace NaN values with the most frequent value for non-numerical columns\n",
    "    # Get the most frequent string value\n",
    "    # Get the most frequent value for the column and replace NaN values with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check (printing the variable dataset) that there are no NaN values left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using pyplot library the dataset feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot a scatter/bubble plot to identify some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the 'Age' attribute on the y-axis\n",
    "\n",
    "\n",
    "# Plot scatter/bubble plot with an attribute on the x-axis. Ypu caan choose what ever attribute you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, the 'Age' attribute in our dataset contains errors, such as improbable values like 150 for age or an age less than 0. To ensure the integrity of our data, we need to perform cleaning by filtering out such rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the condition for the age values (between 0 and 105 years old) [use a boolean condition saved in mask variable]\n",
    "\n",
    "# Apply the condition to the dataset and store the result in the dataset variable (overwrite the previous dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 'Response attribute' (that is in the last column) from the variable dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.iloc` function in Pandas is used for integer-location based indexing. It allows you to select rows and columns from a DataFrame by their integer position, rather than by label. This function provides a way to select data by position, similar to indexing in NumPy arrays.\n",
    "\n",
    "### Syntax\n",
    "\n",
    "```python\n",
    "DataFrame.iloc[row_indexer, column_indexer]\n",
    "```\n",
    "\n",
    "- `row_indexer`: Specifies the rows to select. It can be:\n",
    "  - An integer, e.g., `2`.\n",
    "  - A list or array of integers, e.g., `[1, 3, 5]`.\n",
    "  - A slice object with integers, e.g., `1:4`.\n",
    "  - A boolean array.\n",
    "\n",
    "- `column_indexer`: Specifies the columns to select. It follows the same rules as `row_indexer`.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "data = {'A': [1, 2, 3, 4],\n",
    "        'B': [5, 6, 7, 8],\n",
    "        'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Selecting specific rows and columns using iloc\n",
    "selected_data = df.iloc[1:3, 0:2]\n",
    "print(selected_data)\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "   A  B\n",
    "1  2  6\n",
    "2  3  7\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "- `.iloc` is exclusive of the end index when using slices, similar to Python indexing conventions.\n",
    "- If you want to select specific rows and columns by label instead of position, you should use the `.loc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last column from the dataset (which is the Resposnse attribute) [you can use index -1 in Python] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset and check that the last column has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize age attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "\n",
    "# Normalize the 'age' attribute [using the .fit_transform() function]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age should be in range [0-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset function to check the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Kmedoids Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kmedois clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering is a strategy of hierarchical clustering. Hierarchical clustering (also known as Connectivity based clustering) is a method of cluster analysis which seeks to build a hierarchy of clusters. Hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataset to avoid modifying the original data\n",
    "\n",
    "\n",
    "# Instantiate a LabelEncoder object to encode categorical variables\n",
    "\n",
    "# Iterate through columns of the dataset that have object data type\n",
    "# and encode them using the LabelEncoder\n",
    "\n",
    "\n",
    "# Instantiate a KMedoids clustering model with 3 clusters\n",
    "\n",
    "\n",
    "# Fit the KMedoids clustering model to the encoded dataset\n",
    "# and obtain cluster labels for each data point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the silhouette_score function to evaluate the clustering model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SVD algorithm after performing KMedoids cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These codes use dimensionality reduction technique, SVD, to reduce the dimensionality of the dataset to 3 dimensions, and then visualize the data points in a 3D scatter plot. Each data point is colored according to its assigned cluster label obtained from a clustering algorithm (`cluster_labels.labels_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TruncatedSVD object with 2 components\n",
    "\n",
    "\n",
    "# Fit the TruncatedSVD model to the dataset and transform it\n",
    "# using the cluster labels obtained from the KMedoids clustering\n",
    "\n",
    "\n",
    "# Create a new figure for plotting\n",
    "\n",
    "# Scatter plot of the transformed data with colors representing cluster labels\n",
    "\n",
    "# Set plot title and axis labels\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LDA algorithm after performing KMedoids cluster algorithm and compare the visualization done before with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LDA object with 2 components\n",
    "\n",
    "# Fit the LDA model to the dataset and transform it\n",
    "# using the cluster labels obtained from the KMedoids clustering\n",
    "\n",
    "\n",
    "# Create a new figure for plotting\n",
    "\n",
    "\n",
    "# Scatter plot of the transformed data with colors representing cluster labels\n",
    "\n",
    "# Set plot title and axis labels\n",
    "\n",
    "# Show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply DBSCAN algorithm to the dataset after coverting it into one hot encoding form [use .get_dummies()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm used in machine learning and data mining. It groups together points that are closely packed together based on their density in a high-dimensional space. Unlike other clustering algorithms, DBSCAN doesn't require the number of clusters to be specified in advance. Instead, it defines clusters as continuous regions of high density separated by regions of low density.\n",
    "\n",
    "The key parameters of DBSCAN are:\n",
    "\n",
    "- Epsilon (ε): A distance threshold that determines the neighborhood of a point.\n",
    "- MinPts: The minimum number of points required to form a dense region (cluster).\n",
    "\n",
    "DBSCAN works by iteratively exploring the neighborhood of each point. A point is classified as a core point if it has at least MinPts points within its ε-neighborhood. Core points are then used to expand clusters by adding neighboring points to the same cluster. Points that are not core points themselves but are within the ε-neighborhood of a core point are classified as border points and are included in the cluster. Points that are not core points and don't have enough neighboring points are considered noise and are not assigned to any cluster.\n",
    "\n",
    "DBSCAN is particularly useful for clustering data with irregular shapes and handling noise effectively. It's robust to outliers and doesn't require specifying the number of clusters beforehand, making it suitable for various applications, including spatial data analysis, anomaly detection, and image segmentation. However, choosing appropriate values for ε and MinPts can be challenging and may significantly affect the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataset to avoid modifying the original data\n",
    "\n",
    "\n",
    "# Perform one-hot encoding on the categorical variables in the dataset\n",
    "\n",
    "\n",
    "# Initialize DBSCAN with specified parameters (epsilon=1.0, min_samples=3)\n",
    "\n",
    "\n",
    "# Fit DBSCAN to the one-hot encoded dataset and obtain cluster labels\n",
    "\n",
    "\n",
    "# Analyze the clusters\n",
    "# Calculate the number of clusters and the number of noise points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(dataset_copy, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text files stored in a specific folder, tokenize them, remove stopwords, perform stemming, and then convert them into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix using scikit-learn's TfidfVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the folder path containing the text files\n",
    "\n",
    "# List to hold preprocessed text from each file\n",
    "\n",
    "# List to hold file names\n",
    "\n",
    "# stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# file with the stopwords\n",
    "file_italian_stopwords = open(\"TODO\")\n",
    "italian_stopwords = set(file_italian_stopwords.read().splitlines())\n",
    "\n",
    "# Loop through files in the folder\n",
    "for file_name in os.listdir('''TO COMPILE'''):\n",
    "    # open file\n",
    "\n",
    "        # Read the text from the file\n",
    "\n",
    "        # Tokenization\n",
    "\n",
    "        # Transform to lowercase\n",
    "\n",
    "        # Remove stopwords\n",
    "\n",
    "        # Stemming\n",
    "\n",
    "        # Join the tokens back to form preprocessed text\n",
    "\n",
    "        # Append the preprocessed text to the list\n",
    "\n",
    "        # Append the file name to the list\n",
    "\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply KMeans algorithm to the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is a popular clustering algorithm used in unsupervised machine learning for partitioning a dataset into a predetermined number of clusters. It aims to group similar data points together while maximizing the distance between different clusters. The algorithm iteratively assigns each data point to the nearest cluster centroid and recalculates the centroids based on the mean of the points in each cluster. This process continues until the centroids no longer change significantly, indicating convergence. K-means is sensitive to the initial placement of centroids, and different initializations can lead to different clustering results. Therefore, multiple runs with random initializations are often performed to mitigate this issue. While K-means is computationally efficient and easy to implement, it assumes spherical clusters and struggles with non-linear or irregularly shaped clusters. Additionally, it may not perform well with datasets of varying densities or clusters of unequal sizes. Despite these limitations, K-means remains widely used for clustering tasks in various domains due to its simplicity and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering\n",
    "\n",
    "# Number of clusters\n",
    "\n",
    "# Maximum number of iterations\n",
    "\n",
    "# Obtain the centroids of the clusters\n",
    "\n",
    "# Calculate the cosine similarity between the documents and the cluster centroids\n",
    "\n",
    "# Assign the documents to clusters based on maximum cosine similarity\n",
    "\n",
    "# Print the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
