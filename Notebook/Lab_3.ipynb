{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LAB 03 - Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luca Catalano, Daniele Rege Cambrin and Eleonora Poeta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The purpose of creating this material is to enhance the knowledge of students who are interested in learning how to solve problems presented in laboratory classes using Python. This decision stems from the observation that some students have opted to utilize Python for tackling exam projects in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve these exercises using Python, you need to install Python (version 3.9.6 or later) and some libraries using pip or conda.\n",
    "\n",
    "Here's a list of the libraries needed for this case:\n",
    "\n",
    "- `os`: Provides operating system dependent functionality, commonly used for file operations such as reading and writing files, interacting with the filesystem, etc.\n",
    "- `pandas`: A data manipulation and analysis library that offers data structures and functions to efficiently work with structured data.\n",
    "- `numpy`: A numerical computing library that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "- `matplotlib.pyplot`: A plotting library for creating visualizations like charts, graphs, histograms, etc.\n",
    "- `sklearn`: Machine learning algorithms and tools.\n",
    "- `sklearn_extra`: Additional machine learning algorithms and extensions.\n",
    "- `nltk`: The Natural Language Toolkit, a library for natural language processing tasks such as tokenization, stemming, part-of-speech tagging, and more.\n",
    "- `xlrd`: A Python library used for reading data and formatting information from Excel files (.xls and .xlsx formats). It provides functionality to extract data from Excel worksheets, including cells, rows, columns, and formatting details.\n",
    "\n",
    "You can download Python from [here](https://www.python.org/downloads/) and follow the installation instructions for your operating system.\n",
    "\n",
    "For installing libraries using [pip](https://pip.pypa.io/en/stable/) or [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html), you can use the following commands:\n",
    "\n",
    "- Using pip:\n",
    "  ```\n",
    "  pip install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "- Using conda:\n",
    "  ```\n",
    "  conda install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "Make sure to run these commands in your terminal or command prompt after installing Python. You can also execute them in a cell of a Jupyter Notebook file (`.ipynb`) by starting the command with '!'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file excel named \"UserSmall.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle Missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find if there are missing values in our dataset. \n",
    "\n",
    "Usually in a real dataset the missing values are stored with a nan value. In this case we have ? as missing values representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all we can replace each '?' symbol in a nan value [using .replace()]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values for each column [using .isnumm() and .sum() functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in class there are different methodologies for filling the nan values. Here we will use the average for the numerical data and the most frequent string for non-numerical columns [use .fillna() and mean() function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with the average value for numerical columns\n",
    "    # Get the average value for the column and replace NaN values with it\n",
    "\n",
    "# Replace NaN values with the most frequent value for non-numerical columns\n",
    "    # Get the most frequent string value\n",
    "    # Get the most frequent value for the column and replace NaN values with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check (printing the variable dataset) that there are no NaN values left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using pyplot library the dataset feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot a scatter/bubble plot to identify some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the 'Age' attribute on the y-axis\n",
    "\n",
    "\n",
    "# Plot scatter/bubble plot with an attribute on the x-axis. Ypu caan choose what ever attribute you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, the 'Age' attribute in our dataset contains errors, such as improbable values like 150 for age or an age less than 0. To ensure the integrity of our data, we need to perform cleaning by filtering out such rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the condition for the age values (between 0 and 105 years old) [use a boolean condition saved in mask variable]\n",
    "\n",
    "# Apply the condition to the dataset and store the result in the dataset variable (overwrite the previous dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 'Response attribute' (that is in the last column) from the variable dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.iloc` function in Pandas is used for integer-location based indexing. It allows you to select rows and columns from a DataFrame by their integer position, rather than by label. This function provides a way to select data by position, similar to indexing in NumPy arrays.\n",
    "\n",
    "### Syntax\n",
    "\n",
    "```python\n",
    "DataFrame.iloc[row_indexer, column_indexer]\n",
    "```\n",
    "\n",
    "- `row_indexer`: Specifies the rows to select. It can be:\n",
    "  - An integer, e.g., `2`.\n",
    "  - A list or array of integers, e.g., `[1, 3, 5]`.\n",
    "  - A slice object with integers, e.g., `1:4`.\n",
    "  - A boolean array.\n",
    "\n",
    "- `column_indexer`: Specifies the columns to select. It follows the same rules as `row_indexer`.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "data = {'A': [1, 2, 3, 4],\n",
    "        'B': [5, 6, 7, 8],\n",
    "        'C': [9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Selecting specific rows and columns using iloc\n",
    "selected_data = df.iloc[1:3, 0:2]\n",
    "print(selected_data)\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "   A  B\n",
    "1  2  6\n",
    "2  3  7\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "- `.iloc` is exclusive of the end index when using slices, similar to Python indexing conventions.\n",
    "- If you want to select specific rows and columns by label instead of position, you should use the `.loc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last column from the dataset (which is the Resposnse attribute) [you can use index -1 in Python] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset and check that the last column has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize age attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "\n",
    "# Normalize the 'age' attribute [using the .fit_transform() function]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age should be in range [0-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset function to check the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Kmedoids Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kmedois clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering is a strategy of hierarchical clustering. Hierarchical clustering (also known as Connectivity based clustering) is a method of cluster analysis which seeks to build a hierarchy of clusters. Hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataset to avoid modifying the original data\n",
    "\n",
    "\n",
    "# Instantiate a LabelEncoder object to encode categorical variables\n",
    "\n",
    "# Iterate through columns of the dataset that have object data type\n",
    "# and encode them using the LabelEncoder\n",
    "\n",
    "\n",
    "# Instantiate a KMedoids clustering model with 3 clusters\n",
    "\n",
    "\n",
    "# Fit the KMedoids clustering model to the encoded dataset\n",
    "# and obtain cluster labels for each data point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the silhouette_score function to evaluate the clustering model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualize the distribution of people with Marital-Status \"Divorced\" within the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'marital status = Divorced'\n",
    "\n",
    "# Plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply `get_dummies` to our dataset to convert categorical variables into dummy variables. This allows us to use these variables in our k-Medoids clustering algorithm.\n",
    "\n",
    "`get_dummies` is a function in pandas library used for converting categorical variables into dummy/indicator variables. When applied to a DataFrame column containing categorical data, it creates new binary columns for each category present in the original column. Each binary column indicates whether a particular category is present or not for each row in the dataset.\n",
    "\n",
    "For example, consider a column \"Color\" with categories \"Red\", \"Green\", and \"Blue\". After applying `get_dummies`, the DataFrame will have three new columns: \"Color_Red\", \"Color_Green\", and \"Color_Blue\". For each row, only one of these columns will have a value of 1, indicating the presence of that color, while the others will have a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataset\n",
    "# use .get_dummies()\n",
    "\n",
    "# Apply K-Medoids Algorithm\n",
    "# Instantiate the KMedoids object\n",
    "\n",
    "\n",
    "# Fit the k-medoids model to the dataset_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the silhouette_score function to evaluate the clustering model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the agglomerative clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering is a strategy of hierarchical clustering. Hierarchical clustering (also known as Connectivity based clustering) is a method of cluster analysis which seeks to build a hierarchy of clusters. Hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables to numerical labels\n",
    "\n",
    "\n",
    "# Instantiate a LabelEncoder object\n",
    "\n",
    "\n",
    "# Fit the agglomerative clustering model to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the silhouette_score function to evaluate the clustering model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply `get_dummies` to our dataset to convert categorical variables into dummy variables. This allows us to use these variables in our agglomerative clustering algorithm.\n",
    "\n",
    "`get_dummies` is a function in pandas library used for converting categorical variables into dummy/indicator variables. When applied to a DataFrame column containing categorical data, it creates new binary columns for each category present in the original column. Each binary column indicates whether a particular category is present or not for each row in the dataset.\n",
    "\n",
    "For example, consider a column \"Color\" with categories \"Red\", \"Green\", and \"Blue\". After applying `get_dummies`, the DataFrame will have three new columns: \"Color_Red\", \"Color_Green\", and \"Color_Blue\". For each row, only one of these columns will have a value of 1, indicating the presence of that color, while the others will have a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataset\n",
    "\n",
    "# use .get_dummies()\n",
    "\n",
    "# Apply agglomerative algorithm\n",
    "# Instantiate the agglomerative cluster object\n",
    "\n",
    "\n",
    "# Fit the k-medoids model to the dataset_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cluster algorithm after applying the one hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette score is a metric used to evaluate the quality of clustering in unsupervised learning. It quantifies how similar an object is to its own cluster compared to other clusters. The silhouette score ranges from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the silhouette_score function to evaluate the clustering model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
