{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LAB 01 - Python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luca Catalano, Daniele Rege Cambrin and Eleonora Poeta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The purpose of creating this material is to enhance the knowledge of students who are interested in learning how to solve problems presented in laboratory classes using Python. This decision stems from the observation that some students have opted to utilize Python for tackling exam projects in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve these exercises using Python, you need to install Python (version 3.9.6 or later) and some libraries using pip or conda.\n",
    "\n",
    "Here's a list of the libraries needed for this case:\n",
    "\n",
    "- `os`: Provides operating system dependent functionality, commonly used for file operations such as reading and writing files, interacting with the filesystem, etc.\n",
    "- `pandas`: A data manipulation and analysis library that offers data structures and functions to efficiently work with structured data.\n",
    "- `numpy`: A numerical computing library that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
    "- `matplotlib.pyplot`: A plotting library for creating visualizations like charts, graphs, histograms, etc.\n",
    "- `sklearn`: Machine learning algorithms and tools.\n",
    "- `sklearn_extra`: Additional machine learning algorithms and extensions.\n",
    "- `nltk`: The Natural Language Toolkit, a library for natural language processing tasks such as tokenization, stemming, part-of-speech tagging, and more.\n",
    "- `xlrd`: A Python library used for reading data and formatting information from Excel files (.xls and .xlsx formats). It provides functionality to extract data from Excel worksheets, including cells, rows, columns, and formatting details.\n",
    "\n",
    "You can download Python from [here](https://www.python.org/downloads/) and follow the installation instructions for your operating system.\n",
    "\n",
    "For installing libraries using [pip](https://pip.pypa.io/en/stable/) or [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html), you can use the following commands:\n",
    "\n",
    "- Using pip:\n",
    "  ```\n",
    "  pip install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "- Using conda:\n",
    "  ```\n",
    "  conda install pandas numpy matplotlib nltk scikit-learn xlrd scikit-learn-extra\n",
    "  ```\n",
    "\n",
    "Make sure to run these commands in your terminal or command prompt after installing Python. You can also execute them in a cell of a Jupyter Notebook file (`.ipynb`) by starting the command with '!'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file excel named \"UserSmall.xls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the Excel file using a function integrated into the pandas library, you can use the `pd.read_excel()` function. Rewrite the instruction with the argument as the path of the file to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Jupyter Notebook cell, you can print a subset of the representation by simply calling the name of the variable containing the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle Missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find if there are missing values in our dataset. \n",
    "\n",
    "Usually in a real dataset the missing values are stored with a nan value. In this case we have ? as missing values representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all we can replace each '?' symbol in a nan value [using .replace()]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values for each column [using .isnumm() and .sum() functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in class there are different methodologies for filling the nan values. Here we will use the average for the numerical data and the most frequent string for non-numerical columns [use .fillna() and mean() function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with the average value for numerical columns\n",
    "    # Get the average value for the column and replace NaN values with it\n",
    "\n",
    "# Replace NaN values with the most frequent value for non-numerical columns\n",
    "    # Get the most frequent string value\n",
    "    # Get the most frequent value for the column and replace NaN values with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check (printing the variable dataset) that there are no NaN values left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot using pyplot library the dataset feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot a scatter/bubble plot to identify some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the 'Age' attribute on the y-axis\n",
    "\n",
    "\n",
    "# Plot scatter/bubble plot with an attribute on the x-axis. Ypu caan choose what ever attribute you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, the 'Age' attribute in our dataset contains errors, such as improbable values like 150 for age or an age less than 0. To ensure the integrity of our data, we need to perform cleaning by filtering out such rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the condition for the age values (between 0 and 105 years old) [use a boolean condition saved in mask variable]\n",
    "\n",
    "# Apply the condition to the dataset and store the result in the dataset variable (overwrite the previous dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Discretize some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data discretization is a preprocessing technique used to transform continuous data into discrete intervals or categories. This process involves dividing the continuous range of values into a finite number of intervals, or bins. Discretization is commonly used in data analysis and machine learning tasks to simplify data representation, reduce noise, and improve the performance of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges\n",
    "# Define your own bin edges as needed\n",
    "\n",
    "# Define bin labels\n",
    "\n",
    "# Discretize 'Age' attribute using cut() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text files stored in a specific folder, tokenize them, remove stopwords, perform stemming, and then convert them into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix using scikit-learn's TfidfVectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code could be divided in different sections:\n",
    "\n",
    "#### Download NLTK resources\n",
    "- The code begins by downloading necessary resources from NLTK (Natural Language Toolkit) library, specifically the 'punkt' tokenizer and 'stopwords' corpus.\n",
    "\n",
    "#### Define Folder Path and Initialize Variables\n",
    "- Next, the code defines the folder path containing the text files to be processed.\n",
    "- It initializes two lists: 'preprocessed_texts' to hold preprocessed text from each file, and 'file_names' to store the names of the files.\n",
    "\n",
    "#### Initialize Snowball Stemmer and Define Italian Stopwords\n",
    "- Italian stopwords are defined to remove common and uninformative words from the text. Stopwords are either loaded from NLTK's stopwords corpus or from a custom file.\n",
    "\n",
    "#### Loop Through Files in the Folder\n",
    "- The code iterates through each file in the specified folder.\n",
    "- It reads the content of each file, tokenizes the text into individual words, converts them to lowercase, removes stopwords, and performs stemming on the remaining tokens.\n",
    "- The preprocessed text is then joined back together and appended to the 'preprocessed_texts' list. Additionally, the file name is added to the 'file_names' list.\n",
    "\n",
    "#### TF-IDF Vectorization\n",
    "- The TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer is initialized to convert the preprocessed text data into a matrix of TF-IDF features.\n",
    "- The preprocessed text data is fitted and transformed using the TF-IDF vectorizer, resulting in a TF-IDF matrix.\n",
    "- The TF-IDF matrix is converted into a DataFrame for better visualization, with columns representing unique features extracted from the text and rows corresponding to the files processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the folder path containing the text files\n",
    "\n",
    "# List to hold preprocessed text from each file\n",
    "\n",
    "# List to hold file names\n",
    "\n",
    "# stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# file with the stopwords\n",
    "file_italian_stopwords = open(\"TODO\")\n",
    "italian_stopwords = set(file_italian_stopwords.read().splitlines())\n",
    "\n",
    "# Loop through files in the folder\n",
    "for file_name in os.listdir('''TO COMPILE'''):\n",
    "    # open file\n",
    "\n",
    "        # Read the text from the file\n",
    "\n",
    "        # Tokenization\n",
    "\n",
    "        # Transform to lowercase\n",
    "\n",
    "        # Remove stopwords\n",
    "\n",
    "        # Stemming\n",
    "\n",
    "        # Join the tokens back to form preprocessed text\n",
    "\n",
    "        # Append the preprocessed text to the list\n",
    "\n",
    "        # Append the file name to the list\n",
    "\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better visualization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
